{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Comparaison de différentes méthodes de machine learning sur 3 jeux de données différents. \n",
    "\n",
    "Thomas Renne\n",
    "\n",
    "Code des différents modèles de classification disponible sur [GitHub][https://github.com/RudolphDev/machine_learning]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tr_functions.general import GeneralModel\n",
    "from tr_functions.gaussian import GaussianModel\n",
    "from tr_functions.kppv import KppvModel\n",
    "from tr_functions.parzen import ParzenModel\n",
    "from tr_functions.linear import LinearSeparationModel\n",
    "from tr_functions.bagging import BaggingModel\n",
    "from tr_functions.scikit_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data_tp1_app = GeneralModel.open_file('data/data_tp1_app.txt')\n",
    "split_data_tp1_dec = GeneralModel.open_file('data/data_tp1_dec.txt')\n",
    "\n",
    "split_data_tp2_app = GeneralModel.open_file('data/data_tp2_app.txt')\n",
    "split_data_tp2_dec = GeneralModel.open_file('data/data_tp2_dec.txt')\n",
    "\n",
    "split_data_tp3_app = GeneralModel.open_file('data/data_tp3_app.txt')\n",
    "split_data_tp3_dec = GeneralModel.open_file('data/data_tp3_dec.txt')"
   ]
  },
  {
   "source": [
    "# Analyse visuelle\n",
    "### Dataset 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general = GeneralModel()\n",
    "# general.train_data = split_data_tp1_app\n",
    "# general.test_data = split_data_tp1_dec\n",
    "# general.plot_all_data()"
   ]
  },
  {
   "source": [
    "On remarque que les jeux de données d'apprentissage (croix plus claires) et de test (points sombres) se superposent relativement bien a l'exception de quelques outliers dans chaque classe. Normalement les différents modèles devraient séparer les classes avec une bon niveau de confiance. Dans cet exemple, la séparation des classes peut se faire manuellement. \n",
    "\n",
    "### Dataset 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general = GeneralModel()\n",
    "# general.train_data = split_data_tp2_app\n",
    "# general.test_data = split_data_tp2_dec\n",
    "# general.plot_all_data()"
   ]
  },
  {
   "source": [
    "Contrairement au premier jeu de données, les classes semblent plus diffusent que ce soit pour les jeux d'apprentissage et de test. En revanche on remarque que les classes sont constante entre les deux jeux de données. Les résultats des différents modèles seront moins bon que pour le premier jeu. La séparation des classes peut se faire manuellement avec quelques difficultés.\n",
    "\n",
    "### Dataset 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general = GeneralModel()\n",
    "# general.train_data = split_data_tp3_app\n",
    "# general.test_data = split_data_tp3_dec\n",
    "# general.plot_all_data()"
   ]
  },
  {
   "source": [
    "Ce troisième jeu de données est totalement mélangé. Visuellement on ne peut pas distinguer de classes. En revanche comme pour les deux précédents jeux on remarque les classes d'apprentissage et de tests se supperposent bien. Dans ce cas précis une analyse manuelle est impossible. Il est probable que ce jeu de données donne les moins bon résultats pour les différents modèles. \n",
    "\n",
    "# Estimation gaussienne (euclidienne)\n",
    "\n",
    "### Dataset 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gaussian_model = GaussianModel()\n",
    "# gaussian_model.gaussian_fit_model(split_data_tp1_app)\n",
    "# gaussian_model.print_classes_centers()\n",
    "# gaussian_model.plot_train_data()\n",
    "# gaussian_model.compute_method = \"euclidian\"\n",
    "# gaussian_model.test_model(split_data_tp1_dec)\n",
    "# gaussian_model.print_model_result()\n",
    "# gaussian_model.plot_test_data()\n"
   ]
  },
  {
   "source": [
    "L'analyse du jeu de données d'apprentissage avec la position de chaque classe représenté en par les croix noires montre une répartition bien visible. L'analyse du jeux de test montre un taux de Top 1 de 99,2% et de Top 2 de 100%. On peut donc en déduire que la méthode de l'estimation gaussienne avec distance euclidienne prédit de manière efficace les classes. Sur les 500 classes à prédire 496 sont bien classés.\n",
    "Le deuxième graphique représente le jeu de données tests avec les classes en croix noires. Les couleurs des points représentent les classes prédites et les contours représentent les classes théoriques. On remarque que les 4 classes mal placées sont des classes qui peuvent être considéré comme des outliers, car posisitionné entre deux groupes de classes. \n",
    "\n",
    "### Dataset 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaussian_model = GaussianModel()\n",
    "# gaussian_model.gaussian_fit_model(split_data_tp2_app)\n",
    "# gaussian_model.print_classes_centers()\n",
    "# gaussian_model.plot_train_data()\n",
    "# gaussian_model.compute_method = \"euclidian\"\n",
    "# gaussian_model.test_model(split_data_tp2_dec)\n",
    "# gaussian_model.print_model_result()\n",
    "# gaussian_model.plot_test_data()"
   ]
  },
  {
   "source": [
    "Comme précédement on remarque que les centres de classes sont bien positionnés par rapport aux classes. Comparé au précédent dataset les résultats sont un peu moins bon avec seulement 94% en Top 1. Lorsque l'on regarde la matrice de confusion on remarque que la classe numéro 3 est la classe la moins bien classée. En analysant le graphique on remarque la classe 3 (verte) a de nombreux points proche des centres de classes 5 et 2. De plus on remarque que les 6 points mal classés de la classe 4 sont placé dans la classe 3. La méthode est donc un peu moins bonne lorsque les données se chevauchent.  \n",
    "\n",
    "### Dataset 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaussian_model = GaussianModel()\n",
    "# gaussian_model.gaussian_fit_model(split_data_tp3_app)\n",
    "# gaussian_model.print_classes_centers()\n",
    "# gaussian_model.plot_train_data()\n",
    "# gaussian_model.compute_method = \"euclidian\"\n",
    "# gaussian_model.test_model(split_data_tp3_dec)\n",
    "# gaussian_model.print_model_result()\n",
    "# gaussian_model.plot_test_data()"
   ]
  },
  {
   "source": [
    "Pour le dataset 3 on remarque que les différentes classes sont mélangé car les plages de valeurs sont plus larges. Le résultat en Top 1 est de seulement 73%, et le Top 2 ne monte qu'à 89% soit moins que les deux autres datasets. Lorsque l'on étudie la matrice de confusion, on remarque que la classe la moins bien classé avec seulement 43% de bon est la classe 1. En comparant avec le graphique on remarque que la classe 1 est au centre des quatre autres. Donc ces points ce mélange avec ces classes, comme on peut le voir dans la matrice. Les résultats des autres classes sont meilleurs car elles ont de nombreux points à l'extérieur des classes, donc sans ambiguité. \n",
    "\n",
    "## Distance de mahalanobis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Dataset 1:\")\n",
    "# gaussian_model = GaussianModel()\n",
    "# gaussian_model.gaussian_fit_model(split_data_tp1_app)\n",
    "# gaussian_model.print_classes_centers()\n",
    "# gaussian_model.plot_train_data()\n",
    "# gaussian_model.compute_method = \"mahalanobis\"\n",
    "# gaussian_model.test_model(split_data_tp1_dec)\n",
    "# gaussian_model.print_model_result()\n",
    "# gaussian_model.plot_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Dataset 2:\")\n",
    "# gaussian_model = GaussianModel()\n",
    "# gaussian_model.gaussian_fit_model(split_data_tp2_app)\n",
    "# gaussian_model.print_classes_centers()\n",
    "# gaussian_model.plot_train_data()\n",
    "# gaussian_model.compute_method = \"mahalanobis\"\n",
    "# gaussian_model.test_model(split_data_tp2_dec)\n",
    "# gaussian_model.print_model_result()\n",
    "# gaussian_model.plot_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Dataset 3:\")\n",
    "# gaussian_model = GaussianModel()\n",
    "# gaussian_model.gaussian_fit_model(split_data_tp3_app)\n",
    "# gaussian_model.print_classes_centers()\n",
    "# gaussian_model.plot_train_data()\n",
    "# gaussian_model.compute_method = \"mahalanobis\"\n",
    "# gaussian_model.test_model(split_data_tp3_dec)\n",
    "# gaussian_model.print_model_result()\n",
    "# gaussian_model.plot_test_data()"
   ]
  },
  {
   "source": [
    "La deuxième analyse réalisée sur les 3 datasets avec l'utilisation de la distance de Mahalanobis n'améliore pas les résultats de Top 1 et un tout petit peu ceux de Top 2 pour le dataset numéro 3. Donc utiliser une distance de Mahalanobis peu être recommandé pour des données moins bien discriminé. Et couplé avec une méthode de bagging. \n",
    "\n",
    "## Comparaison avec scikit learn\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "# x_train, y_train, x_test, y_test = create_train_test_data_format_scikit(split_data_tp1_app, split_data_tp1_dec)\n",
    "# gpc1 = GaussianProcessClassifier()\n",
    "# gpc1.fit(x_train, y_train)\n",
    "# score = gpc1.score(x_test, y_test)\n",
    "# print(\"Score dataset 1:\")\n",
    "# print(score)\n",
    "\n",
    "# x_train, y_train, x_test, y_test = create_train_test_data_format_scikit(split_data_tp2_app, split_data_tp2_dec)\n",
    "# gpc2 = GaussianProcessClassifier()\n",
    "# gpc2.fit(x_train, y_train)\n",
    "# score = gpc2.score(x_test, y_test)\n",
    "# print(\"Score dataset 2:\")\n",
    "# print(score)\n",
    "\n",
    "# x_train, y_train, x_test, y_test = create_train_test_data_format_scikit(split_data_tp3_app, split_data_tp3_dec)\n",
    "# gpc3 = GaussianProcessClassifier()\n",
    "# gpc3.fit(x_train, y_train)\n",
    "# score = gpc3.score(x_test, y_test)\n",
    "# print(\"Score dataset 3:\")\n",
    "# print(score)\n"
   ]
  },
  {
   "source": [
    "# K plus proches voisins\n",
    "\n",
    "## Analyse à 1ppv\n",
    "\n",
    "### Dataset 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kppv = KppvModel()\n",
    "# kppv.k = 1\n",
    "# kppv.vote_method = \"unanimous\"\n",
    "# kppv.compute_kppv(split_data_tp1_app, split_data_tp1_dec)\n",
    "# kppv.print_model_result()\n",
    "# kppv.plot_test_data()"
   ]
  },
  {
   "source": [
    "L'analyse par kppv à 1 voisin, montre un bon taux de classement pour le premier jeu de données. En effet on remarque que le taux de bonne classification est de 99%. La matrice de confusion montre que 4 valeurs on été mal classées. Le graphique indique que ces 4 valeurs ont des positions ambigues entre deux classes. \n",
    "\n",
    "### Dataset 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kppv = KppvModel()\n",
    "# kppv.k = 1\n",
    "# kppv.vote_method = \"unanimous\"\n",
    "# kppv.compute_kppv(split_data_tp2_app, split_data_tp2_dec)\n",
    "# kppv.print_model_result()\n",
    "# kppv.plot_test_data()"
   ]
  },
  {
   "source": [
    "Les résultats de classifications avec un 1 plus proche voisin montre un taux de bonne classification moins bon que celui du dataset 1 et aussi moins bon qu'avec la methode d'estimation gaussienne (92% de top 1 contre 94% avec l'estimation). Comme pour la précédente méthode, la classe la moins bien classé est la 3. Donc un plus proche voisin ne permet pas distinguer les points se trouvant plus vers d'autres classes, comme le montre le graphique de résultats de tests.\n",
    "\n",
    "### Dataset 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kppv = KppvModel()\n",
    "# kppv.k = 1\n",
    "# kppv.vote_method = \"unanimous\"\n",
    "# kppv.compute_kppv(split_data_tp3_app, split_data_tp3_dec)\n",
    "# kppv.print_model_result()\n",
    "# kppv.plot_test_data()"
   ]
  },
  {
   "source": [
    "La classification du troisième dataset avec un plus proche voisin est mauvaise. En effet seul 65% des points sont bien classés. Ce résultat est moins bon que celui de l'estimation gaussienne (73%). Comme pour cette méthode la classe la moins classé est la classe 1. Cette classe est centrale donc tout les points se mélangent en partie avec les autres. \n",
    "\n",
    "## kppv avec cross-validation\n",
    "\n",
    "Nous allons maintenant essayer de trouver le meilleur K possible pour chaque dataset par cross-validation. Un fois ce k trouvé, on va l'utiliser avec le jeu de données de test. Et voir si un K > 1 améliore le taux de bonne classification. Pour cela nous allons comparer deux méthodes pour chaque jeux de données, un vote à la majorité et à l'unanimité.\n",
    "\n",
    "### Dataset 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kppv = KppvModel()\n",
    "# kppv.vote_method = \"majority\"\n",
    "# kppv.get_k_cross_validation(split_data_tp1_app, 10, 5)\n",
    "# kppv.compute_kppv(split_data_tp1_app, split_data_tp1_dec)\n",
    "# kppv.print_model_result()\n",
    "# kppv.plot_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kppv = KppvModel()\n",
    "# kppv.vote_method = \"unanimous\"\n",
    "# kppv.get_k_cross_validation(split_data_tp1_app, 10, 5)\n",
    "# kppv.compute_kppv(split_data_tp1_app, split_data_tp1_dec)\n",
    "# kppv.print_model_result()\n",
    "# kppv.plot_test_data()"
   ]
  },
  {
   "source": [
    "Pour le premier jeu de données, chercher le meilleur K avec la cross-validation n'est pas nécéssaire. En effet, ce jeu de données est suffisament bien discriminé. Donc le meilleur k trouvé est 1. Sachant que pour k = 1, les votes à la majorité et à l'unanimité sont identiques on ne peut pas conclure sur le efficacité. On obtient donc les même résultats que précédement.\n",
    "\n",
    "### Dataset 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kppv = KppvModel()\n",
    "# kppv.vote_method = \"majority\"\n",
    "# kppv.get_k_cross_validation(split_data_tp2_app, 50, 5)\n",
    "# kppv.compute_kppv(split_data_tp2_app, split_data_tp2_dec)\n",
    "# kppv.print_model_result()\n",
    "# kppv.plot_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kppv = KppvModel()\n",
    "# kppv.vote_method = \"unanimous\"\n",
    "# kppv.get_k_cross_validation(split_data_tp2_app, 50, 5)\n",
    "# kppv.compute_kppv(split_data_tp2_app, split_data_tp2_dec)\n",
    "# kppv.print_model_result()\n",
    "# kppv.plot_test_data()"
   ]
  },
  {
   "source": [
    "Pour l'analyse du deuxième jeu de données, on remarque une grosse disparité entre les votes à la majorité et à l'unanimité. Dans les deux cas, la cross-validation trouve une valeur de K optimal = **TODO**. Les K supérieur ont des taux d'erreur supérieur ce qui pourrait être dû au sur-apprentissage. Lorsque que l'on compare les résultats sur le jeu de test, on remarque que pour le vote à la majorité le taux de Top 1 a augmenté en passant de 92% à **TODO**%. En revanche pour le vote à l'unanimité, le taux de Top 1 est de seulement **TODO** avec un taux d'erreur de **TODO**. Ce taux Top 1 faible est dû aux cas ou il y a plusieurs classes possible trouvé. Le vote à l'unanimité ne peux pas gérer ces cas alors que celui à la majorité oui. L'étude des tableaux ou des graphs montre des résultats similaire entre les deux. En effet j'ai décidé de remplir les tableaux sans tenir compte des erreurs. Dans tous les cas le tableau est rempli avec la valeur majoritaire. La différence entre les tableaux est donc expliqué uniquement par la différence de K. Dans ce cas on remarque qu'un K **TODO** donne un meilleur résultats.\n",
    "\n",
    "### Dataset 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kppv = KppvModel()\n",
    "# kppv.vote_method = \"majority\"\n",
    "# kppv.get_k_cross_validation(split_data_tp3_app, 50, 5)\n",
    "# kppv.compute_kppv(split_data_tp3_app, split_data_tp3_dec)\n",
    "# kppv.print_model_result()\n",
    "# kppv.plot_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kppv = KppvModel()\n",
    "# kppv.vote_method = \"unanimous\"\n",
    "# kppv.get_k_cross_validation(split_data_tp3_app, 50, 5)\n",
    "# kppv.compute_kppv(split_data_tp3_app, split_data_tp3_dec)\n",
    "# kppv.print_model_result()\n",
    "# kppv.plot_test_data()"
   ]
  },
  {
   "source": [
    "Pour ce dernier jeu de données, sont similaire à ceux du deuxième jeu de données. En effet, les valeurs de K trouvés par cross-validation sont similaire, comme précédement les taux Top 1 sont meilleurs pour le vote à la majorité. Et encore une fois lorsque l'on compare les résultats du tableau et du graphique on remarque qu'un K plus grand permet de meilleurs résultats donc ce n'est pas le sur-apprentissage qui explique l'augmentation du taux d'erreur lors de la cross-validation. \n",
    "\n",
    "La méthode de classification par Kppv ne semble pas plus performante que l'estimation gaussienne alors que le modèle est plus lent et plus complexe. \n",
    "\n",
    "## Comparaison avec scikit-learn\n",
    "Nous allons utiliser le kppv de scikit learn avec la même valeur de k et comparer les résultats. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# x_train, y_train, x_test, y_test = create_train_test_data_format_scikit(split_data_tp1_app, split_data_tp1_dec)\n",
    "# kppv1 = KNeighborsClassifier(n_neighbors=1)\n",
    "# kppv1.fit(x_train, y_train)\n",
    "# print(\"kppv dataset 1 :\")\n",
    "# print(kppv1.score(x_test, y_test))\n",
    "\n",
    "# x_train, y_train, x_test, y_test = create_train_test_data_format_scikit(split_data_tp2_app, split_data_tp2_dec)\n",
    "# kppv1 = KNeighborsClassifier(n_neighbors=5)\n",
    "# kppv1.fit(x_train, y_train)\n",
    "# print(\"kppv dataset 2 :\")\n",
    "# print(kppv1.score(x_test, y_test))\n",
    "\n",
    "# x_train, y_train, x_test, y_test = create_train_test_data_format_scikit(split_data_tp3_app, split_data_tp3_dec)\n",
    "# kppv1 = KNeighborsClassifier(n_neighbors=10)\n",
    "# kppv1.fit(x_train, y_train)\n",
    "# print(\"kppv dataset 3 :\")\n",
    "# print(kppv1.score(x_test, y_test))"
   ]
  },
  {
   "source": [
    "# Methode de Parzen\n",
    "Comme précédement pour le kppv, la méthode de Parzen est un modèle paramétrique. Dans ce cas le paramètre est noté h et correspond à la largeur de la fonction uniforme ou à l'écart-type de la fonction gaussienne. Comme précédement, cet hyper-paramètre doit être trouvé. Nous allons donc utiliser la méthode de la cross-validation. En effet étudier le modèle avec un h aléatoire n'est pas pertinent. \n",
    "## Noyau uniforme\n",
    "Pour le noyau uniforme j'ai décidé de tester tout les h possible entre 0,1 et 5 avec un pas de 0,1. J'ai décidé d'aller j'usqu'a 5 car cela représente environ la moitité de la taille des classes.\n",
    "### Dataset 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parzen = ParzenModel()\n",
    "# h_list = np.arange(0.1, 5, 0.1)\n",
    "# parzen.method = \"uniform\"\n",
    "# parzen.get_h_cross_validation(split_data_tp1_app, h_list, 5)\n",
    "# parzen.compute_parzen(split_data_tp1_app, split_data_tp1_dec)\n",
    "# parzen.print_model_result()\n",
    "# parzen.plot_test_data()"
   ]
  },
  {
   "source": [
    "L'analyse de l'hyperparamètre h montre que pour la méthode uniforme plus le h est important plus le taux d'erreur est faible avec un plateau atteint aux environ de 2. Finalement le taux de bonne classification est identiques aux autres modèles de classification. \n",
    "### Dataset 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parzen = ParzenModel()\n",
    "# h_list = np.arange(0.1, 5, 0.1)\n",
    "# parzen.method = \"uniform\"\n",
    "# parzen.get_h_cross_validation(split_data_tp2_app, h_list, 5)\n",
    "# parzen.compute_parzen(split_data_tp2_app, split_data_tp2_dec)\n",
    "# parzen.print_model_result()\n",
    "# parzen.plot_test_data()"
   ]
  },
  {
   "source": [
    "Malgré un jeu de données plus complexe pour le jeu de données 2 on remarque un profil de taux d'erreur en fonction de H similaire avec un plateau commencant vers 2. Les résultats de classification top 1 semblent meilleurs que les autres modèles (95,8% contre environ 94,8% pour les autres modèles). Comme pour tout les modèles on remarque que la classe 3 est toujours la plus mal classé car se trouvant au centre des autres. \n",
    "\n",
    "### Dataset 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parzen = ParzenModel()\n",
    "# h_list = np.arange(0.1, 5, 0.1)\n",
    "# parzen.method = \"uniform\"\n",
    "# parzen.get_h_cross_validation(split_data_tp3_app, h_list, 5)\n",
    "# parzen.compute_parzen(split_data_tp3_app, split_data_tp3_dec)\n",
    "# parzen.print_model_result()\n",
    "# parzen.plot_test_data()"
   ]
  },
  {
   "source": [
    "Comme pour les deux autres datasets, le taux d'erreur varie de la même façon en fonction du h. On obtient un plateau au environ de 2. Les résultats de Top 1 montre un taux d'erreur similaire aux autres modèles, pas d'amélioration. La classe est 1 est très mal classé, ce qui s'explique par la position de cette classe. lorsque l'on regarde les données on remarque que les points se mélangent avec les autres classes. Le graphique montre en revanche des points mal classé de façon totalement illogique, tel que les points rouges en bas qui sont trouvé en bleu. \n",
    "## Noyau Gaussien\n",
    "Nous allons maintenant étudier la méthode de Parzen avec un noyau gaussien. Contrairement au noyau uniforme, les résultats sont plus quantitatif et non binaire. Ceci va normalement permettre d'afiner le modèle. Nous allons encore une fois utiliser un hyperparamètre h. Le h correspond cette fois ci à l'écart type de la gaussienne utilisé. Comme précédement nous allons estimer le h par cross-validation.\n",
    "### Dataset 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parzen = ParzenModel()\n",
    "# h_list = np.arange(0.1, 5, 0.1)\n",
    "# parzen.method = \"gaussian\"\n",
    "# parzen.get_h_cross_validation(split_data_tp1_app, h_list, 5)\n",
    "# parzen.compute_parzen(split_data_tp1_app, split_data_tp1_dec)\n",
    "# parzen.print_model_result()\n",
    "# parzen.plot_test_data()"
   ]
  },
  {
   "source": [
    "Pour le premier jeux de données, le meilleur h trouvé est le minimum soit 0,1. Plus le h augmentais moins le niveau d'erreur est faible donc nous utilisons le minimum. Finalement le taux de Top 1 ainsi que la matrice de confusion et le graphique final ne montre pas d'amélioration du niveau de classification.\n",
    "\n",
    "### Dataset 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parzen = ParzenModel()\n",
    "# h_list = np.arange(0.1, 5, 0.1)\n",
    "# parzen.method = \"gaussian\"\n",
    "# parzen.get_h_cross_validation(split_data_tp2_app, h_list, 5)\n",
    "# parzen.compute_parzen(split_data_tp2_app, split_data_tp2_dec)\n",
    "# parzen.print_model_result()\n",
    "# parzen.plot_test_data()"
   ]
  },
  {
   "source": [
    "L'analyse du deuxième jeu de données montre que pour des données plus mélangés, un h plus élévé est necessaire. On remarque en revanche un plateau aux alentours de 2. Avec cet hyperparamètre, le taux de classification Top 1 est finalement similaire à tous les autres. Il n'y a pas de de plus-value à utiliser une modèle plus complexe que l'uniforme.  \n",
    "\n",
    "### Dataset 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parzen = ParzenModel()\n",
    "# h_list = np.arange(0.1, 5, 0.1)\n",
    "# parzen.method = \"gaussian\"\n",
    "# parzen.get_h_cross_validation(split_data_tp3_app, h_list, 5)\n",
    "# parzen.compute_parzen(split_data_tp3_app, split_data_tp3_dec)\n",
    "# parzen.print_model_result()\n",
    "# parzen.plot_test_data()"
   ]
  },
  {
   "source": [
    "La cross-validation faite sur ce troisième jeu de données montre que un hyperparamètre optimal aux alentour de 1,5. Contrairement au dernier jeu de données, lorsque l'on augmente la valeur de h au dessus de 1,5 le taux d'erreur augmente significativement. Avec cet h, le taux de classification et la matrice de confusion montre des résultats identiques aux autres modèles.  \n",
    "\n",
    "## Comparaison avec scikit learn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Score Dataset 1\n0.994\nScore Dataset 2\n0.946\nScore Dataset 3\n0.686\n"
     ]
    }
   ],
   "source": [
    "# print(\"Score Dataset 1\")\n",
    "# print(find_best_score_kernel(split_data_tp1_app, split_data_tp1_dec))\n",
    "# print(\"Score Dataset 2\")\n",
    "# print(find_best_score_kernel(split_data_tp2_app, split_data_tp2_dec))\n",
    "# print(\"Score Dataset 3\")\n",
    "# print(find_best_score_kernel(split_data_tp3_app, split_data_tp3_dec))"
   ]
  },
  {
   "source": [
    "# Séparation linéaire\n",
    "## Comparaison un contre un\n",
    "Nous allons maintenant étudier le modèle de la séparation linéaire. Pour cela nous allons commencer par tester des séparations linéaires entre chaques classes. Pour la décision finale nous compterons combien de fois chaque classe à été trouvé pour chaque séapration. \n",
    "### Dataset 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# linear = LinearSeparationModel()\n",
    "# model = linear.linear_train(split_data_tp1_app, is_converging=True, one_vs_all=False) \n",
    "# linear.print_model()\n",
    "# linear.test_linear_model(split_data_tp1_dec, model)\n",
    "# linear.plot_linear_data()\n",
    "# linear.print_model_result()\n",
    "# linear.plot_test_data()"
   ]
  },
  {
   "source": [
    "Ce premier jeu de données à des données qui peuvent être séparées linéairement. Donc nous testons la méthode convergente. L'analyse des différents graphiques de séparations linéaires entre les classes montre que pour les classes avec une séparation verticale la droite est relativement bien placé alors que pour les graphs avec une séparation linéaire horizontale, les droites ne sont pas très bonne. Ceci peut s'expliqué par la méthode. Nous validons une droite dès que les données sont toutes bien classées. Nous ne faisons pas d'epochs pour permettre d'améliorer la distance entre les points et la droite. Finalement le taux de Top 1 de bonne classification est similaire aux autres modèles. Donc il est lui aussi pertinent pour un jeu de données relativement bien séparé. \n",
    "\n",
    "### Dataset 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear = LinearSeparationModel()\n",
    "# linear.epochs = 1000\n",
    "# model = linear.linear_train(split_data_tp2_app, is_converging=False, one_vs_all=False)\n",
    "# linear.print_model()\n",
    "# linear.test_linear_model(split_data_tp2_dec, model)\n",
    "# linear.plot_linear_data()\n",
    "# linear.print_model_result()\n",
    "# linear.plot_test_data()"
   ]
  },
  {
   "source": [
    "Le deuxième jeu de données n'est pas convergent. En effet on ne peut pas trouver de droites qui séparent toutes les classes entre elles sans aucune erreur. Nous décidons donc de fixer le nombre d'epoch réalisées pour améliorer le modèle. J'ai commencé par réaliser 100 epochs. L'observation des droites produites me semblaient bonnes sauf pour celle séparant les classes 2 et 3. En effet la droite de séparation se trouve à gauche des deux jeux de données. J'ai donc décidé de monter le nombre d'epoch à 1000. On remarque que ça ne permet pas d'améliorer le modèle. Les classes 2 et 3 sont toujours mal classées. \n",
    "\n",
    "L'analyse des résultats montre un taux de classification Top 1 inférieur aux autres méthodes, seulement 80%. La matrice de convolution et le graphique des résultats confirment bien que le problème provient bien de la différenciation entre les classes 2 et 3.\n",
    "\n",
    "### Dataset 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear = LinearSeparationModel()\n",
    "# linear.epochs = 1000\n",
    "# model = linear.linear_train(split_data_tp3_app, is_converging=False, one_vs_all=False)\n",
    "# linear.print_model()\n",
    "# linear.test_linear_model(split_data_tp3_dec, model)\n",
    "# linear.plot_linear_data()\n",
    "# linear.print_model_result()\n",
    "# linear.plot_test_data()"
   ]
  },
  {
   "source": [
    "Le jeu de données 3 est le plus complexe, avec les classes les plus mélangés. Donc l'analyse des différentes classes et de leurs séparation linéaire est relativement mauvaise comme attendu. Pourtant après de multiples test, augmenter le nombre d'époch n'améliore pas le modèle. On peut penser que les poids se trouvent dans des minima locaux qui ne leurs permettent pas de trouver les minima globaux. \n",
    "\n",
    "Finalement l'analyse des résultats de ce modèle est cohérent avec la prédiction. En effet il est moins bon que les précédents avec seulement 55% de bonne classification Top 1. En revanche en étudiant les différentes classes on remarque qu'il y a une grande disparité. Les classes 3 et 5 sont bien classées alors que les 1 et 2 ont des score très mauvais. On remarque que pour ces deux classes, le modèle trouve majoritairement les classes 3 et 5. On peut donc en déduire que ce modèle donne trop de poid à ces deux classes. \n",
    "\n",
    "## Comparaison un contre tous\n",
    "Après avoir essayé de comparer chaque classe une à une nous allons tenter de voir si nous pouvons trouver une séparation linéaire pouvant séparer une classe contre toutes les autres. Pour cela nous allons supprimer la classe centrale de chaque jeu de données. Car chevauchant régulièrement les autres classes elle poserait problème.\n",
    "\n",
    "### Dataset 1\n",
    "D'après la représentation graphique de l'analyse visuelle on remarque que la classe centrale est la classe 5. Nous supprimons donc cette classe et étudions la séparation des 4 classes restantes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_app_data = GeneralModel.remove_one_class_from_data(split_data_tp1_app, '5')\n",
    "# clean_dec_data = GeneralModel.remove_one_class_from_data(split_data_tp1_dec, '5')\n",
    "# linear = LinearSeparationModel()\n",
    "# linear.epochs = 1000\n",
    "# model = linear.linear_train(clean_app_data, is_converging=False, one_vs_all=True)\n",
    "# linear.print_model()\n",
    "# linear.test_linear_model(clean_dec_data, model)\n",
    "# linear.print_model_result()\n",
    "# linear.plot_test_data()"
   ]
  },
  {
   "source": [
    "Une première tentative avec une méthode convergente n'a pas pu aboutir comme pour la séparation un contre un. Donc le modèle est non convergent avec 1000 epochs comme pour les autres jeux de données. Le jeu de données 1 peut relativement bien être séparé par des droites entre les classes et le reste. En revanche le taux de bonne classification reste moins bon que les autres modèles, malgré le retrait de la classe centrale. Cependant la comparaison n'est pas totalement pertinente car nous comparons un jeu de données moins complexe que celui utilisé dans les autres méthodes. La matrice de confusion et le graphique des résultats montre que peux de points sont mal classées mais qu'une bonne partie d'entre eux ne sont pas classable car présent dans deux classes ou plus. \n",
    "\n",
    "### Dataset 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_app_data = GeneralModel.remove_one_class_from_data(split_data_tp2_app, '3')\n",
    "# clean_dec_data = GeneralModel.remove_one_class_from_data(split_data_tp2_dec, '3')\n",
    "# linear = LinearSeparationModel()\n",
    "# linear.epochs = 1000\n",
    "# model = linear.linear_train(clean_app_data, is_converging=False, one_vs_all=True)\n",
    "# linear.print_model()\n",
    "# linear.test_linear_model(clean_dec_data, model)\n",
    "# linear.print_model_result()\n",
    "# linear.plot_test_data()"
   ]
  },
  {
   "source": [
    "Comme précédemment nous utilisons 1000 epochs pour trouver le meilleur modèle. Le taux de Top 1 est toujours plus faible que celui des autres modèles. Lorsque l'on étudie la matrice de confusion et le graphique des résultats, on remarque que la classe 2 est très mal classé. La plupart des droites séparent bien les classes des autres. En revanche pour la classe 4, la droite ne dissocie pas les classes 2 et 4. Ceci implique que les points de classe 2 sont tous mal classés. Malgré des tests pour augmenter le nombre d'epochs, les résultats ne s'améliorent pas.\n",
    "\n",
    "### Dataset 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_app_data = GeneralModel.remove_one_class_from_data(split_data_tp3_app, '1')\n",
    "# clean_dec_data = GeneralModel.remove_one_class_from_data(split_data_tp3_dec, '1')\n",
    "# linear = LinearSeparationModel()\n",
    "# linear.epochs = 1000\n",
    "# model = linear.linear_train(clean_app_data, is_converging=False, one_vs_all=True)\n",
    "# linear.print_model()\n",
    "# linear.test_linear_model(clean_dec_data, model)\n",
    "# linear.print_model_result()\n",
    "# linear.plot_test_data()"
   ]
  },
  {
   "source": [
    "Pour ce dernier jeu de données le taux de bonne classification est très faible. En effet le taux est de seulement 35%. Lorsque l'on regarde la matrice de confusion et le graphique, on remarque qu'une classe est bien classé et que toutes les autres sont mal classées. Avec même la classe 2 qui a un taux de bonne classification de 0. \n",
    "\n",
    "Pour des jeux de données complexe comme les datasets 2 et 3, la séparation linéaire n'est donc pas recommandé. \n",
    "\n",
    "## Comparaison avec scikit learn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "kppv dataset 1 :\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'kppv1' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-4347f187729f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msvc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kppv dataset 1 :\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkppv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'kppv1' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "x_train, y_train, x_test, y_test = create_train_test_data_format_scikit(split_data_tp1_app, split_data_tp1_dec)\n",
    "svc1 = LinearSVC()\n",
    "svc1.fit(x_train, y_train)\n",
    "print(\"kppv dataset 1 :\")\n",
    "print(svc1.score(x_test, y_test))"
   ]
  },
  {
   "source": [
    "## Bagging\n",
    "Le modèle de séparation linéaire n'est pas optimal comparé aux autres méthodes. Nous allons donc utiliser une méthode de bootstrap nommé Bagging afin d'essayer d'améliorer les taux de bonne classification de ce modèle. \n",
    "### Dataset 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bagging = BaggingModel()\n",
    "# bagging.get_nb_bagging_cv(split_data_tp1_app, cv=5, max_n=10, is_converging=True)\n",
    "# bagging.test_bagging_linear_model(split_data_tp1_app, split_data_tp1_dec)\n",
    "# bagging.compute_bagging_results()\n",
    "# bagging.print_model_result()\n",
    "# bagging.plot_test_data()"
   ]
  },
  {
   "source": [
    "Nous analysons le premier Dataset avec du bagging de spéaration linéaire. Lors de l'analyse précédente nous avons vu que le modèle est convergent. Nous allons donc continuer à utiliser cette méthode. Afin de connaitre le nombre de bagging optimal nous allons utiliser la cross validation comme précédement. Nous testons le modèle entre 1 et 10 n. Nous ne pouvons pas trop aller plus loin en nombre de n car le temps de calcul est très long. \n",
    "\n",
    "Avec cette cross-validation nous trouvons un nombre de bagging optimal de TODO. On remarque que le taux Top 1 est identique à celui des autres modèles. la matrice de confusion et le graphique confirme ce résultat. \n",
    "\n",
    "### Dataset 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bagging = BaggingModel()\n",
    "# bagging.get_nb_bagging_cv(split_data_tp2_app, cv=5, max_n=10, is_converging=False)\n",
    "# bagging.test_bagging_linear_model(split_data_tp2_app, split_data_tp2_dec, is_converging=False)\n",
    "# bagging.compute_bagging_results()\n",
    "# bagging.print_model_result()\n",
    "# bagging.plot_test_data()"
   ]
  },
  {
   "source": [
    "Ce deuxième jeu de données est non convergent comme vu précédemment donc nous allons utiliser un bagging sur un modèle linéaire non convergent. Le nombre d'epochs est fixé à 10. Cette valeur est relativement faible mais si nous utilisons le même nombre d'epochs que précédemment, le temps d'execution serait trop lent. \n",
    "\n",
    "La cross-validation du bagging montre que le nombre de bootstrap n'est pas très important car le taux d'erreur ne varie pas beaucoup entre chaque valeur de n. En revanche pour le taux de bonne classification en Top 1 on retrouve un résultat similaire que pour les anciens modèle. Ceci permet donc d'améliorer le modèle de séparation linéaire. La matrice de confusion et le graphique confirment ces résultats. \n",
    "\n",
    "### Dataset 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bagging = BaggingModel()\n",
    "# bagging.get_nb_bagging_cv(split_data_tp3_app, cv=5, max_n=10, is_converging=False)\n",
    "# bagging.test_bagging_linear_model(split_data_tp3_app, split_data_tp3_dec, is_converging=False)\n",
    "# bagging.compute_bagging_results()\n",
    "# bagging.print_model_result()\n",
    "# bagging.plot_test_data()"
   ]
  },
  {
   "source": [
    "Le troisième jeu de données est analysé avec la méthode du bagging. Nous utilisons les mêmes paramètres que pour le deuxième. Nous obtenons des résultats similaire à ceux des autres modèles. Le bagging permet donc d'améliorer le taux de classification pour le modèle linéaire. \n",
    "\n",
    "Nous allons donc essayer de voir si cette méthode de bagging permet d'améliorer une autre méthode ayant eu de bons résultats. Soit le kppv avec vote à la majorité. Nous allons utiliser les même K obtenu par cross-validation lors de l'analyse du Kppv. De même nous utiliserons le nombre de bagging trouvé pour la séparation linéaire.\n",
    "\n",
    "## Bagging de Kppv"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bagging = BaggingModel()\n",
    "# bagging.nb_models=5\n",
    "# bagging.test_bagging_kppv_model(split_data_tp1_app, split_data_tp1_dec, k=1)\n",
    "# bagging.compute_bagging_results()\n",
    "# bagging.print_model_result()\n",
    "# bagging.plot_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bagging = BaggingModel()\n",
    "# bagging.nb_models=5\n",
    "# bagging.test_bagging_kppv_model(split_data_tp2_app, split_data_tp2_dec, k=1)\n",
    "# bagging.compute_bagging_results()\n",
    "# bagging.print_model_result()\n",
    "# bagging.plot_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bagging = BaggingModel()\n",
    "# bagging.nb_models=5\n",
    "# bagging.test_bagging_kppv_model(split_data_tp3_app, split_data_tp3_dec, k=1)\n",
    "# bagging.compute_bagging_results()\n",
    "# bagging.print_model_result()\n",
    "# bagging.plot_test_data()"
   ]
  },
  {
   "source": [
    "# Conclusion\n",
    "||Dataset 1|Dataset 2|Dataset 3|\n",
    "|----|-----|-----|-----|\n",
    "|estimation gaussienne (euclidienne)| 99,2% | 94,6% | 72,8% |\n",
    "|estimation gaussienne (mahalanobis)| 99,2% | 94,6% | 72,8% |\n",
    "|1ppv| 99,2% | 92% | 64,6% |\n",
    "|kppv (unanimité)| 99,2% | 59% | 12,6% |\n",
    "|kppv (majorité)| 99,6% | 94,6% | 68,6% |\n",
    "|Parzen (uniform)| 99,2% | 95,2% | 70,4% |\n",
    "|Parzen (gaussienne)| 99,6% | 94,4% | 69,2% |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}