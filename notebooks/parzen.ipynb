{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data_tp1_app = open_file('../data/data_tp1_app.txt')\n",
    "split_data_tp1_dec = open_file('../data/data_tp1_dec.txt')\n",
    "\n",
    "split_data_tp2_app = open_file('../data/data_tp2_app.txt')\n",
    "split_data_tp2_dec = open_file('../data/data_tp2_dec.txt')\n",
    "\n",
    "split_data_tp3_app = open_file('../data/data_tp3_app.txt')\n",
    "split_data_tp3_dec = open_file('../data/data_tp3_dec.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "84 0.84 0.84\n",
      "79 0.79 1.63\n",
      "83 0.83 2.46\n",
      "83 0.83 3.29\n",
      "78 0.78 4.07\n",
      "0.8140000000000001\n",
      "=============\n",
      "75 0.75 0.75\n",
      "69 0.69 1.44\n",
      "74 0.74 2.1799999999999997\n",
      "75 0.75 2.9299999999999997\n",
      "65 0.65 3.5799999999999996\n",
      "0.716\n",
      "=============\n",
      "71 0.71 0.71\n",
      "63 0.63 1.3399999999999999\n",
      "69 0.69 2.03\n",
      "70 0.7 2.7299999999999995\n",
      "62 0.62 3.3499999999999996\n",
      "0.6699999999999999\n",
      "=============\n",
      "70 0.7 0.7\n",
      "64 0.64 1.3399999999999999\n",
      "65 0.65 1.9899999999999998\n",
      "70 0.7 2.6899999999999995\n",
      "62 0.62 3.3099999999999996\n",
      "0.6619999999999999\n",
      "=============\n",
      "68 0.68 0.68\n",
      "59 0.59 1.27\n",
      "58 0.58 1.85\n",
      "73 0.73 2.58\n",
      "60 0.6 3.18\n",
      "0.636\n",
      "=============\n",
      "66 0.66 0.66\n",
      "59 0.59 1.25\n",
      "56 0.56 1.81\n",
      "70 0.7 2.51\n",
      "61 0.61 3.1199999999999997\n",
      "0.6239999999999999\n",
      "=============\n",
      "60 0.6 0.6\n",
      "55 0.55 1.15\n",
      "50 0.5 1.65\n",
      "71 0.71 2.36\n",
      "62 0.62 2.98\n",
      "0.596\n",
      "=============\n",
      "56 0.56 0.56\n",
      "54 0.54 1.1\n",
      "53 0.53 1.6300000000000001\n",
      "69 0.69 2.3200000000000003\n",
      "65 0.65 2.97\n",
      "0.5940000000000001\n",
      "=============\n",
      "54 0.54 0.54\n",
      "51 0.51 1.05\n",
      "57 0.57 1.62\n",
      "67 0.67 2.29\n",
      "65 0.65 2.94\n",
      "0.588\n",
      "=============\n",
      "[0.01, 0.8140000000000001]\n"
     ]
    }
   ],
   "source": [
    "def get_h_cross_validation(app_data, h_list, cv, method):\n",
    "    best_h = [0, 0]\n",
    "    df = pd.DataFrame(app_data)\n",
    "    shuffled = df.sample(frac=1)\n",
    "    cut_dfs = np.array_split(shuffled, cv)  \n",
    "    for h in h_list:\n",
    "        sum_error = 0\n",
    "        for i in range(cv):\n",
    "            df_cv = cut_dfs[i]\n",
    "            df_train = df.drop(df_cv.index)\n",
    "            df_cv = df_cv.values.tolist()\n",
    "            df_train = df_train.values.tolist()\n",
    "            count_top_1 = 0\n",
    "            for line in df_cv:\n",
    "                if method == \"uniform\":\n",
    "                    results = get_dict_uniform_value(line[1:], app_data, h)\n",
    "                elif method == \"gaussian\":\n",
    "                    results = get_dict_gaussian_value(line[1:], app_data, h)\n",
    "                else:\n",
    "                    print(\"The given method: {} is not implemented.\\n You must choose between \\\"uniform\\\" and \\\"gaussian\\\"\".format(method))\n",
    "                    exit()\n",
    "                temp = max(results.values()) \n",
    "                res = [key for key in results if results[key] == temp]\n",
    "                if res[0] == line[0]:\n",
    "                    count_top_1 = count_top_1 + 1\n",
    "            error_rate = count_top_1/len(df_cv)\n",
    "            sum_error = sum_error + error_rate\n",
    "            print(count_top_1, error_rate, sum_error)\n",
    "        print(sum_error/cv)\n",
    "        print(\"=============\")\n",
    "        if best_h[1] < sum_error/cv:\n",
    "            best_h[0] = h\n",
    "            best_h[1] = sum_error/cv\n",
    "    return best_h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniform_val(app_point, dec_point, h):\n",
    "    result = 0\n",
    "    dist = compute_one_euclidian_dist(app_point, dec_point)\n",
    "    if (dist > -h) & (dist < h):\n",
    "        result = 1\n",
    "    return result\n",
    "\n",
    "def get_top_n_decision_parzen(n, theo_class, dists):\n",
    "    dists_sorted = sorted(dists.items(), key = lambda kv: kv[1], reverse=True)\n",
    "    cut_dists =dists_sorted[0:n]\n",
    "    top_n_result = False\n",
    "    for dist in cut_dists:\n",
    "        if theo_class == dist[0]:\n",
    "           top_n_result =True\n",
    "    return top_n_result\n",
    "\n",
    "def get_dict_uniform_value(point, data, h):\n",
    "    result_dict = {}\n",
    "    class_num = get_unique_class_num(data)\n",
    "    COUNT_CLASS = 100\n",
    "    for one_class in class_num:\n",
    "        count_uniform = 0\n",
    "        for app_point in data:\n",
    "            if app_point[0] == one_class:\n",
    "                count_uniform = count_uniform + get_uniform_val(app_point[1:], point, h)\n",
    "        result_dict[one_class] = count_uniform/COUNT_CLASS\n",
    "    return result_dict\n",
    "\n",
    "def update_confusion_matrix(conf_matrix, line_class, scores_dict):\n",
    "    row_num = int(line_class) - 1\n",
    "    temp = max(scores_dict.values()) \n",
    "    res = [key for key in scores_dict if scores_dict[key] == temp] \n",
    "    col_num = int(res[0]) -1\n",
    "    conf_matrix[row_num, col_num] = conf_matrix[row_num, col_num] + 1\n",
    "\n",
    "def compute_uniform_parzen(app_data, dec_data, h):\n",
    "    count_top_1 = 0\n",
    "    count_top_2 = 0\n",
    "    classes_num =get_unique_class_num(app_data)\n",
    "    conf_matrix = np.zeros((len(classes_num), len(classes_num)))\n",
    "    for line in dec_data:\n",
    "        results = get_dict_uniform_value(line[1:], app_data, h)\n",
    "        if get_top_n_decision_parzen(1, line[0], results):\n",
    "            count_top_1 = count_top_1 + 1\n",
    "        if get_top_n_decision_parzen(2, line[0], results):\n",
    "            count_top_2 = count_top_2 + 1\n",
    "        update_confusion_matrix(conf_matrix, line[0], results)\n",
    "    conf_matrix = transform_matrix_to_df(conf_matrix, classes_num)\n",
    "    print_decision_model_result(len(app_data), len(dec_data), count_top_1/len(dec_data), count_top_2/len(dec_data), conf_matrix=conf_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gaussian_val(app_point, dec_point, h):\n",
    "    result = 0\n",
    "    dist = compute_one_euclidian_dist(app_point, dec_point)\n",
    "    result = (1 / (h * math.sqrt(2 * math.pi)))* math.exp((-(dist)**2)/(2*(h**2)))\n",
    "    return result\n",
    "\n",
    "def get_dict_gaussian_value(point, data, h):\n",
    "    result_dict = {}\n",
    "    class_num = get_unique_class_num(data)\n",
    "    COUNT_CLASS = 100\n",
    "    for one_class in class_num:\n",
    "        count_uniform = 0\n",
    "        for app_point in data:\n",
    "            if app_point[0] == one_class:\n",
    "                count_uniform = count_uniform + get_gaussian_val(app_point[1:], point, h)\n",
    "        result_dict[one_class] = count_uniform/COUNT_CLASS\n",
    "    return result_dict\n",
    "\n",
    "def compute_gaussian_parzen(app_data, dec_data, h):\n",
    "    count_top_1 = 0\n",
    "    count_top_2 = 0\n",
    "    classes_num =get_unique_class_num(app_data)\n",
    "    conf_matrix = np.zeros((len(classes_num), len(classes_num)))\n",
    "    for line in dec_data:\n",
    "        results = get_dict_gaussian_value(line[1:], app_data, h)\n",
    "        if get_top_n_decision_parzen(1, line[0], results):\n",
    "            count_top_1 = count_top_1 + 1\n",
    "        if get_top_n_decision_parzen(2, line[0], results):\n",
    "            count_top_2 = count_top_2 + 1\n",
    "        update_confusion_matrix(conf_matrix, line[0], results)\n",
    "    conf_matrix = transform_matrix_to_df(conf_matrix, classes_num)\n",
    "    print_decision_model_result(len(app_data), len(dec_data), count_top_1/len(dec_data), count_top_2/len(dec_data), conf_matrix=conf_matrix)\n"
   ]
  },
  {
   "source": [
    "## Question  1 \n",
    "### noyau uniforme"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First Data TP1\n",
      "\tResults :\n",
      "\t----------------\n",
      "\tNumber of elements for the learning step :  500\n",
      "\tNumber of elements for the decision step :  500\n",
      "\t----------------\n",
      "\n",
      "\tTop results :\n",
      "\t----------------\n",
      "\tTop 1 rate :  0.642\n",
      "\tTop 2 rate :  0.978\n",
      "\t----------------\n",
      "\n",
      "\tConfusion matrix :\n",
      "\t----------------\n",
      "╒════╤═════╤═════╤═════╤═════╤═════╕\n",
      "│    │   1 │   2 │   3 │   4 │   5 │\n",
      "╞════╪═════╪═════╪═════╪═════╪═════╡\n",
      "│  1 │  36 │  63 │   0 │   0 │   1 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  2 │  40 │  56 │   0 │   0 │   4 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  3 │   0 │   0 │  95 │   5 │   0 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  4 │   0 │   0 │  57 │  39 │   4 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  5 │   1 │   0 │   0 │   4 │  95 │\n",
      "╘════╧═════╧═════╧═════╧═════╧═════╛\n",
      "\t----------------\n",
      "\tExecution time : 0.3470134735107422 seconds\n",
      "\n",
      "==========================\n",
      "\n",
      "First Data TP2\n",
      "\tResults :\n",
      "\t----------------\n",
      "\tNumber of elements for the learning step :  500\n",
      "\tNumber of elements for the decision step :  500\n",
      "\t----------------\n",
      "\n",
      "\tTop results :\n",
      "\t----------------\n",
      "\tTop 1 rate :  0.596\n",
      "\tTop 2 rate :  0.926\n",
      "\t----------------\n",
      "\n",
      "\tConfusion matrix :\n",
      "\t----------------\n",
      "╒════╤═════╤═════╤═════╤═════╤═════╕\n",
      "│    │   1 │   2 │   3 │   4 │   5 │\n",
      "╞════╪═════╪═════╪═════╪═════╪═════╡\n",
      "│  1 │  87 │   0 │   3 │   0 │  10 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  2 │   0 │  64 │   7 │  29 │   0 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  3 │   3 │   2 │  77 │  11 │   7 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  4 │   0 │  56 │  12 │  32 │   0 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  5 │  52 │   0 │  10 │   0 │  38 │\n",
      "╘════╧═════╧═════╧═════╧═════╧═════╛\n",
      "\t----------------\n",
      "\tExecution time : 0.3403441905975342 seconds\n",
      "\n",
      "==========================\n",
      "\n",
      "First Data TP3\n",
      "\tResults :\n",
      "\t----------------\n",
      "\tNumber of elements for the learning step :  500\n",
      "\tNumber of elements for the decision step :  500\n",
      "\t----------------\n",
      "\n",
      "\tTop results :\n",
      "\t----------------\n",
      "\tTop 1 rate :  0.386\n",
      "\tTop 2 rate :  0.728\n",
      "\t----------------\n",
      "\n",
      "\tConfusion matrix :\n",
      "\t----------------\n",
      "╒════╤═════╤═════╤═════╤═════╤═════╕\n",
      "│    │   1 │   2 │   3 │   4 │   5 │\n",
      "╞════╪═════╪═════╪═════╪═════╪═════╡\n",
      "│  1 │  34 │  13 │  20 │  18 │  15 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  2 │  16 │  41 │  41 │   0 │   2 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  3 │  23 │  35 │  39 │   0 │   3 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  4 │  20 │   2 │   3 │  39 │  36 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  5 │  11 │   0 │   5 │  44 │  40 │\n",
      "╘════╧═════╧═════╧═════╧═════╧═════╛\n",
      "\t----------------\n",
      "\tExecution time : 0.33426904678344727 seconds\n",
      "\n",
      "==========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "h = 0.5\n",
    "print(\"First Data TP1\")\n",
    "start_time = time.time()\n",
    "compute_uniform_parzen(split_data_tp1_app, split_data_tp1_dec, h)\n",
    "print(\"\\tExecution time : %s seconds\\n\" % (time.time() - start_time))\n",
    "print(\"==========================\\n\")\n",
    "\n",
    "print(\"First Data TP2\")\n",
    "start_time = time.time()\n",
    "compute_uniform_parzen(split_data_tp2_app, split_data_tp2_dec, h)\n",
    "print(\"\\tExecution time : %s seconds\\n\" % (time.time() - start_time))\n",
    "print(\"==========================\\n\")\n",
    "\n",
    "print(\"First Data TP3\")\n",
    "start_time = time.time()\n",
    "compute_uniform_parzen(split_data_tp3_app, split_data_tp3_dec, h)\n",
    "print(\"\\tExecution time : %s seconds\\n\" % (time.time() - start_time))\n",
    "print(\"==========================\\n\")"
   ]
  },
  {
   "source": [
    "### Noyau gaussien"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.5\n",
    "print(\"First Data TP1\")\n",
    "start_time = time.time()\n",
    "compute_gaussian_parzen(split_data_tp1_app, split_data_tp1_dec, h)\n",
    "print(\"\\tExecution time : %s seconds\\n\" % (time.time() - start_time))\n",
    "print(\"==========================\\n\")\n",
    "\n",
    "print(\"First Data TP2\")\n",
    "start_time = time.time()\n",
    "compute_gaussian_parzen(split_data_tp2_app, split_data_tp2_dec, h)\n",
    "print(\"\\tExecution time : %s seconds\\n\" % (time.time() - start_time))\n",
    "print(\"==========================\\n\")\n",
    "\n",
    "print(\"First Data TP3\")\n",
    "start_time = time.time()\n",
    "compute_gaussian_parzen(split_data_tp3_app, split_data_tp3_dec, h)\n",
    "print(\"\\tExecution time : %s seconds\\n\" % (time.time() - start_time))\n",
    "print(\"==========================\\n\")"
   ]
  },
  {
   "source": [
    "## Question 2\n",
    "### Noyau uniforme avec cross-validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09]\n",
      "First Data TP1\n",
      "0.8860000000000001\n",
      "0.828\n",
      "0.7979999999999999\n",
      "0.788\n",
      "0.7859999999999999\n",
      "0.776\n",
      "0.748\n",
      "0.726\n",
      "0.7380000000000001\n",
      "\tResults :\n",
      "\t----------------\n",
      "\tNumber of elements for the learning step :  500\n",
      "\tNumber of elements for the decision step :  500\n",
      "\t----------------\n",
      "\n",
      "\tTop results :\n",
      "\t----------------\n",
      "\tTop 1 rate :  0.338\n",
      "\tTop 2 rate :  0.57\n",
      "\t----------------\n",
      "\n",
      "\tConfusion matrix :\n",
      "\t----------------\n",
      "╒════╤═════╤═════╤═════╤═════╤═════╕\n",
      "│    │   1 │   2 │   3 │   4 │   5 │\n",
      "╞════╪═════╪═════╪═════╪═════╪═════╡\n",
      "│  1 │  78 │  21 │   0 │   0 │   1 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  2 │  86 │  13 │   0 │   0 │   1 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  3 │  49 │   0 │  32 │  19 │   0 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  4 │  54 │   0 │  29 │  17 │   0 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  5 │  67 │   0 │   0 │   4 │  29 │\n",
      "╘════╧═════╧═════╧═════╧═════╧═════╛\n",
      "\t----------------\n",
      "\tThe best h used is 0.01 with a error rate = 0.8860000000000001\n",
      "\tExecution time : 4.010457515716553 seconds\n",
      "\n",
      "==========================\n",
      "\n",
      "First Data TP2\n",
      "0.8720000000000001\n",
      "0.7999999999999999\n",
      "0.756\n",
      "0.7300000000000001\n",
      "0.7300000000000001\n",
      "0.732\n",
      "0.73\n",
      "0.7220000000000001\n",
      "0.718\n",
      "\tResults :\n",
      "\t----------------\n",
      "\tNumber of elements for the learning step :  500\n",
      "\tNumber of elements for the decision step :  500\n",
      "\t----------------\n",
      "\n",
      "\tTop results :\n",
      "\t----------------\n",
      "\tTop 1 rate :  0.312\n",
      "\tTop 2 rate :  0.482\n",
      "\t----------------\n",
      "\n",
      "\tConfusion matrix :\n",
      "\t----------------\n",
      "╒════╤═════╤═════╤═════╤═════╤═════╕\n",
      "│    │   1 │   2 │   3 │   4 │   5 │\n",
      "╞════╪═════╪═════╪═════╪═════╪═════╡\n",
      "│  1 │  82 │   0 │   1 │   0 │  17 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  2 │  57 │  23 │   8 │  12 │   0 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  3 │  64 │   2 │  27 │   5 │   2 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  4 │  62 │  25 │   4 │   9 │   0 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  5 │  80 │   0 │   5 │   0 │  15 │\n",
      "╘════╧═════╧═════╧═════╧═════╧═════╛\n",
      "\t----------------\n",
      "\tThe best h used is 0.01 with a error rate = 0.8720000000000001\n",
      "\tExecution time : 4.001327991485596 seconds\n",
      "\n",
      "==========================\n",
      "\n",
      "First Data TP3\n",
      "0.8140000000000001\n",
      "0.716\n",
      "0.6699999999999999\n",
      "0.662\n",
      "0.636\n",
      "0.624\n",
      "0.596\n",
      "0.594\n",
      "0.5880000000000001\n",
      "\tResults :\n",
      "\t----------------\n",
      "\tNumber of elements for the learning step :  500\n",
      "\tNumber of elements for the decision step :  500\n",
      "\t----------------\n",
      "\n",
      "\tTop results :\n",
      "\t----------------\n",
      "\tTop 1 rate :  0.254\n",
      "\tTop 2 rate :  0.504\n",
      "\t----------------\n",
      "\n",
      "\tConfusion matrix :\n",
      "\t----------------\n",
      "╒════╤═════╤═════╤═════╤═════╤═════╕\n",
      "│    │   1 │   2 │   3 │   4 │   5 │\n",
      "╞════╪═════╪═════╪═════╪═════╪═════╡\n",
      "│  1 │  56 │   9 │  15 │   9 │  11 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  2 │  66 │  18 │  13 │   0 │   3 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  3 │  61 │  16 │  22 │   0 │   1 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  4 │  65 │   3 │   4 │  19 │   9 │\n",
      "├────┼─────┼─────┼─────┼─────┼─────┤\n",
      "│  5 │  52 │   2 │   4 │  30 │  12 │\n",
      "╘════╧═════╧═════╧═════╧═════╧═════╛\n",
      "\t----------------\n",
      "\tThe best h used is 0.01 with a error rate = 0.8140000000000001\n",
      "\tExecution time : 3.9746713638305664 seconds\n",
      "\n",
      "==========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "h_list = np.arange(0.2, 2, 0.1)\n",
    "print(h_list)\n",
    "print(\"First Data TP1\")\n",
    "start_time = time.time()\n",
    "best_h = get_h_cross_validation(split_data_tp1_app, h_list, 5, \"uniform\")\n",
    "compute_uniform_parzen(split_data_tp1_app, split_data_tp1_dec, best_h[0])\n",
    "print(\"\\tThe best h used is {} with a error rate = {}\".format(best_h[0], best_h[1]))\n",
    "print(\"\\tExecution time : %s seconds\\n\" % (time.time() - start_time))\n",
    "print(\"==========================\\n\")\n",
    "\n",
    "print(\"First Data TP2\")\n",
    "start_time = time.time()\n",
    "best_h = get_h_cross_validation(split_data_tp2_app, h_list, 5, \"uniform\")\n",
    "compute_uniform_parzen(split_data_tp2_app, split_data_tp2_dec, best_h[0])\n",
    "print(\"\\tThe best h used is {} with a error rate = {}\".format(best_h[0], best_h[1]))\n",
    "print(\"\\tExecution time : %s seconds\\n\" % (time.time() - start_time))\n",
    "print(\"==========================\\n\")\n",
    "\n",
    "print(\"First Data TP3\")\n",
    "start_time = time.time()\n",
    "best_h = get_h_cross_validation(split_data_tp3_app, h_list, 5, \"uniform\")\n",
    "compute_uniform_parzen(split_data_tp3_app, split_data_tp3_dec, best_h[0])\n",
    "print(\"\\tThe best h used is {} with a error rate = {}\".format(best_h[0], best_h[1]))\n",
    "print(\"\\tExecution time : %s seconds\\n\" % (time.time() - start_time))\n",
    "print(\"==========================\\n\")"
   ]
  },
  {
   "source": [
    "### Noyau gaussien avec cross-validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_list = np.arange(0.1, 2, 0.1)\n",
    "print(\"First Data TP1\")\n",
    "start_time = time.time()\n",
    "best_h = get_h_cross_validation(split_data_tp1_app, h_list, 5, \"gaussian\")\n",
    "compute_gaussian_parzen(split_data_tp1_app, split_data_tp1_dec, best_h[0])\n",
    "print(\"\\tThe best k used is {} with a error rate = {}\".format(best_h[0], best_h[1]))\n",
    "print(\"\\tExecution time : %s seconds\\n\" % (time.time() - start_time))\n",
    "print(\"==========================\\n\")\n",
    "\n",
    "print(\"First Data TP2\")\n",
    "start_time = time.time()\n",
    "best_h = get_h_cross_validation(split_data_tp2_app, h_list, 5, \"gaussian\")\n",
    "compute_gaussian_parzen(split_data_tp2_app, split_data_tp2_dec, best_k[0])\n",
    "print(\"\\tThe best k used is {} with a error rate = {}\".format(best_k[0], best_k[1]))\n",
    "print(\"\\tExecution time : %s seconds\\n\" % (time.time() - start_time))\n",
    "print(\"==========================\\n\")\n",
    "\n",
    "print(\"First Data TP3\")\n",
    "start_time = time.time()\n",
    "best_h = get_h_cross_validation(split_data_tp3_app, h_list, 5, \"gaussian\")\n",
    "compute_gaussian_parzen(split_data_tp3_app, split_data_tp3_dec, best_k[0])\n",
    "print(\"\\tThe best k used is {} with a error rate = {}\".format(best_k[0], best_k[1]))\n",
    "print(\"\\tExecution time : %s seconds\\n\" % (time.time() - start_time))\n",
    "print(\"==========================\\n\")"
   ]
  }
 ]
}