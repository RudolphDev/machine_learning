{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd0916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1",
   "display_name": "Python 3.8.6 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data_tp1_app = open_file('../data/data_tp1_app.txt')\n",
    "split_data_tp1_dec = open_file('../data/data_tp1_dec.txt')\n",
    "\n",
    "split_data_tp2_app = open_file('../data/data_tp2_app.txt')\n",
    "split_data_tp2_dec = open_file('../data/data_tp2_dec.txt')\n",
    "\n",
    "split_data_tp3_app = open_file('../data/data_tp3_app.txt')\n",
    "split_data_tp3_dec = open_file('../data/data_tp3_dec.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.994\n"
     ]
    }
   ],
   "source": [
    "def create_hyperplans_classes(train_data):\n",
    "    hyper_plans_classes = []\n",
    "    classes = get_unique_class_num(train_data)\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(i+1, len(classes)):\n",
    "            hyper_plans_classes.append((classes[i], classes[j]))\n",
    "    return hyper_plans_classes\n",
    "\n",
    "def predict_one_line_perceptron(line, weights):\n",
    "    total = 0\n",
    "    for i in range(len(line)):\n",
    "        total += line[i] * weights[i]\n",
    "    total += weights[-1]\n",
    "    return total\n",
    "\n",
    "def linear_perceptron_one_epoch(train_data, weights):\n",
    "    count = 0\n",
    "    for index, row in train_data.iterrows():\n",
    "        temp_line = row.tolist()\n",
    "        line = list(map(float, temp_line))\n",
    "        # Step 1\n",
    "        if line[-1] == -1:\n",
    "            point_list = [ -x for x in line[:-1]]\n",
    "            point_list.append(line[-1])\n",
    "        else:\n",
    "            point_list = line\n",
    "        # Step 2\n",
    "        if np.dot(weights, point_list) <= 0:\n",
    "            weights = [weights[i] + point_list[i] for i in range(len(weights))]  \n",
    "            count += 1\n",
    "    return weights, count\n",
    "\n",
    "def linear_perceptron_two_classes_converge(train_data):\n",
    "    weights = np.zeros(len(train_data.columns))\n",
    "    count = 1\n",
    "    while count != 0:\n",
    "        weights, count = linear_perceptron_one_epoch(train_data, weights)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def linear_perceptron_two_classes_non_converge(train_data, epochs):\n",
    "    weights = np.zeros(len(train_data.columns))\n",
    "    final_weights = []\n",
    "    best_count = None\n",
    "    for n in range(epochs):\n",
    "        weights, count = linear_perceptron_one_epoch(train_data, weights)\n",
    "        if best_count == None:\n",
    "            best_count = count\n",
    "        if best_count >= count:\n",
    "            final_weights = weights\n",
    "            best_count = count\n",
    "        \n",
    "    return final_weights, best_count\n",
    "\n",
    "\n",
    "def linear_train(train_data, is_converging:bool = True):\n",
    "    hyper_plan_list = create_hyperplans_classes(train_data)\n",
    "    data_pd = pd.DataFrame(train_data)\n",
    "    results = {}\n",
    "    for hyper_plan in hyper_plan_list:\n",
    "        # Subset with data of the classes split by the hyperplans\n",
    "        hyperplan_data = data_pd[(data_pd[0] == hyper_plan[0]) | (data_pd[0] == hyper_plan[1])].copy()\n",
    "        # Transform the classes to 1 and -1\n",
    "        hyperplan_data[3] = hyperplan_data[0].apply(lambda row: 1 if row==hyper_plan[0] else -1)\n",
    "\n",
    "        del hyperplan_data[0]\n",
    "        if is_converging:\n",
    "            weights = linear_perceptron_two_classes_converge(hyperplan_data)\n",
    "        else:\n",
    "            weights = linear_perceptron_two_classes_non_converge(hyperplan_data, 5)\n",
    "        results[hyper_plan] = weights\n",
    "    return results\n",
    "\n",
    "def test_linear_model(test_data, wheighted_hp:dict):\n",
    "    count = 0\n",
    "    for line in test_data:\n",
    "        line = [float(val) for val in line]\n",
    "        predict_classes = []\n",
    "        for key, value in wheighted_hp.items():\n",
    "            res = predict_one_line_perceptron(line[1:], value)\n",
    "            if res >= 0:\n",
    "                predict_classes.append(key[0])\n",
    "            else:\n",
    "                predict_classes.append(key[1])\n",
    "        max_predict_class = get_n_max_occurence_from_list(predict_classes, 1)\n",
    "        #print(max_predict_class, line[0])\n",
    "        if float(max_predict_class) == float(line[0]):\n",
    "            count += 1\n",
    "    print(count/len(test_data))\n",
    "\n",
    "\n",
    "weighted_hp = linear_train(split_data_tp1_app, is_converging=True)\n",
    "test_linear_model(split_data_tp1_dec, weighted_hp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}